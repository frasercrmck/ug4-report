\section{Code Generation}

Now that we have extracted all of the necessary information about the Dalvik execuatable file to be translated, we can continue to the code generation step of the compilation process. At this stage of the toolchain we traverse this file representation and build the components that LLVM uses to generate code. The outcome of this stage will be an LLVM module of the given Dalvik executable file. The `Module' is an LLVM construct that represents the top level structure in an LLVM program. It is essentially a translation unit of the original program or a combination of several translation units merged together by the linker. In our case it will be a join of a translation of the original Dalvik program with a custom LLVM Java library.

We can subdivide the task of translation into the 3 key areas that make up the resulting LLVM module:

\begin{itemize}
	\item Global Variables
	\item Structs
	\item Functions
\end{itemize}

We shall now examine these sections, how they tie with the original Dalvik program, and the methods used to generate them.

\subsection*{Global Variables}

Java, and by extension Dalvik, doesn't feature global variables. However, this area of the LLVM module is used for static class fields, as well as strings.

\subsection*{Structs}

Structs are analogous to the structs present in the C programming language; they are structured types that combine a set of fields into a single object. Much like the C structs (and unlike those found in C++), they are not permitted to have functions as struct members, nor do they directly support inheritance.

Structs are also the closest LLVM comes to having `classes' in the object-oriented sense. We need to manipulate these structs into representing the classes found in our Dalvik program. Java, being an object-oriented language, bases its entire paradigm around these two omitted features. Fortunately there are ways around these shortcomings. Instead, methods are implemented globally in LLVM, and are not directly associated with an objects, although they are passed a `this' pointer if they are non-static. We shall see this later, in the section on methods. 

Inheritance is effectively achieved through the same paradigm a C programmer might use to accomplish the same goal:

\lstset{
	language=C,
	basicstyle=\small,
	stringstyle=\ttfamily
}

\begin{lstlisting}[frame=single, numbers=left, numberstyle=\tiny, title=C code]
typedef struct {
    // base members
} Base;

typedef struct {
    Base base;  
    // derived members   
} Derived;

...

Derived *d;
Base *b = (Base *)d;
\end{lstlisting}

\begin{lstlisting}[frame=single, numbers=left, numberstyle=\tiny, title= LLVM IR]
%Base = type { /* base members */ }
%Derived = type { %Base, /* derived members */ }

...

%0 = alloca %Derived
%1 = bitcast %Derived* %0 to %Base*
\end{lstlisting}

We can see at line 8 of the C code - and at line 2 of the LLVM bytecode - that each struct features as its first field an instance of the super class. This way it can be cast to this class at runtime, as we do at lines 15 and 7 of each respective code sample.

Another similarity between C structs and their LLVM counterparts is their inability to store incomplete types such as arrays with no given length. Java classes, on the other hand, are allowed to have variable-length arrays as class members, their length being determined at run time. To accomplish this in C and in LLVM we use a must use a few tricks to achieve the same functionality. We shall detail this approach later in the chapter.

\subsection*{Functions}

As previously noted, functions in LLVM modules are not treated as members of a class, rather they are global and are called with an instance of a class passed as a parameter. Functions make up the majority of the LLVM module, as they are where the program instructions are defined.

LLVM intermediate representation is written in static single assignment (SSA) form. This property states that each variable is assigned exactly once, with variables being split into versions, with every definition receiving its own definition. The motive behind SSA form is that it simplifies and improves the results of various compiler optimizations, by more explicitly stating data dependencies. The Dalvik executable format, however, is register-based, so we have no need for SSA form while generating code. Our values are held in a fixed set of registers, and when a value is written to, we can no longer access that value, nor do we need to.

To this end, we instead allocate mutable variables on the stack and use load and store instructions to access them. In LLVM, all memory accesses are explicit with load/store instructions. Every read of a value then becomes a load from the stack, and each update of a variable becomes a store to the stack. This paradigm introduces a problem, however; we now introduce a lot of stack traffic for simple and commonplace operations. This is evidently a major performance issue. The LLVM optimizer has a solution to this problem: the `mem2reg' pass. This optimization promotes memory accesses to register variables and tidies up the loose ends. This is a high-performance optimization that is used by major LLVM frontends such as clang and llvm-gcc\footnotemark \footnotetext[1]{http://llvm.org/docs/tutorial/LangImpl7.html\#memory}. We therefore rely on this optimization pass to keep the generated code performing well.


