\section{Dalvik}
\label{sec:dalvik}

Android features at its core the Dalvik; the application process virtual machine. Programs are typically written in a dialect of the Java programming language, and are then compiled to Java bytecode, as standard. In order to be readable by the Dalvik virtual machine, the .class files are converted into the Dalvik executable (.dex) format. Multiple .class files consitute a single Dalvik executable file. The Dalvik executable format is designed to be run on devices constained in processor and memory speed.

\subsection*{Platform}

Whilst Dalvik's application model is a Java-like, it bears no relationship with any Java standard. The fundamental differences between the two platforms become apparent when we look under the hood and examine the instruction sets and the class of machines under which each application platform is defined.

The standard JVM is stack-based. Operations remove inputs from the stack and push result(s) back onto the stack. One stack level can hold any type (char to float), and double and long values need two consecutive stack levels.

Dalvik, on the other hand, is a more traditional register-based machine. The virtual machine holds a set of virtual registers: theoretically up to 64k, although most application programs will use far fewer than the first 256. There are some similarities, however, in that, like the Java stack machine, one register can hold any type, and both double and long values need to be stored in two consecutive registers.

The advantages and disadvantages of using a register machine over a stack-machine-based approach are numerous, and is a widely contested topic in the field. One argument for the use of a register machine is that is more closely mimics current processors, which are almost always regsiter-based machines. In this way, it can be simpler to create effective optimizations for register-based code quickly. Additionally, a stack-based machine will likely perform many more memory operations for the popping and push of stack values, which is likely to slow down performance. Studies have shown that a registered-based architecture requires an average of 47\% fewer executed VM instructions than stack-based architecture\cite{vmshowdown}, and that register code is 25\% larger than the equivalence stack machine code\cite{vmshowdown}. However, the cost of fetching more VM instructions on account of larger code size involves only 1.07\% extra real machine loads per virtual machine instruction\cite{vmshowdown}, a negligable result. The study concluded by reporting that the overall performance of the register-based virtual machine is that it takes 26.5\% less time to execute standard benchmarks, on average\cite{vmshowdown}.

Stack-based virtual machines are simpler and the instructions are more compact, as the need to decode and encode operand values is removed. A high-performance stack-based virtual machine like the Java virtual machine, is unlikely to rely on memory accesses to access its stack values and will instead promote areas of the stack to register values, and so the performance gap is not so significant as it may seem in the \naive approach. Additionally, the need to write register allocation routines is removed, an NP-complete problem in itself\cite{chaitin82}.

\subsection*{Interpreter}

At the heart of Dalvik is an interpreter, which fetches bytecode instructions and executes them. These bytecode instructions are read from a Dalvik Executable File (which is detailed in Section \ref{sec:dex}), and read by a host-specific interpreter. While most interpeters have a history of poor performance compared with traditional statically-compiled code, Android's virtual machine features a highly-tuned interpreter, which Google claim is twice as fast as a traditional Java interpreter\cite{android_22}. Additionally, less than $1/3$ of application time is actually spent in the interpreter, as operating sytem and performance-critical library code is statically compiled to native code, in order to achieve maximal performance. As a result, for most applications the interpreter's performance is good enough. However, for compute-intensive applications the interpreter is a major performance bottleneck.

Google sought to mitigate this problem: a partial solution was the release of the Android Native Development Kit (NDK) which allows Android applications to call out to natively-compiled methods\cite{android_22}. The other solution was the introduction of a just-in-time compiler.

\subsection*{Just-in-time Compilation}

As of version 2.2 Dalvik has featured a just-in-time compiler\cite{android_22}. The just-in-time compiler translates byte code to optmized native code at run time. More specifically, it interprets the Dalvik bytecodes, finds the most frequently run sections, pulls them out and compiles and optimizes them in native code, then stores them in a `translation cache'. The next time such a hot section of code is run it is directly execeuted in native code.

While there a many varieties of JIT compiler, Android's had to meet the constraints that mobile computing imposes. Its main criteria were that it had to feature:

\begin{itemize}
    \item minimal memory usage
    \item quick delivery of performance boost
    \item smooth transition between interpretation and compiled code
\end{itemize}

In order to save on memory usage, Dalvik opted for a trace-based just-in-time compiler. Instead of analysing hot sections of interpreted code, then compiling entire methods natively, the Android opted for a different approach. A method-based JIT compiler provides certain benefits, such as a larger optimization window, but it fails to take into account the behaviour of the code within the method. In an experiment performed by the Android team on a key Dalvik application, it was found that hot methods accounted for 8\% of the program code, whereas hot traces only accounted for a quarter of those methods: 2\% of the entire application\cite{android_22}. A method-based JIT compiler would be storing the entire block of compiled method code, with only 25\% of that actually worthy of just-in-time compilation. A trace-based approach here offers the best speed vs. space tradeoff by only compiling the very most frequently executed sections of application code.

As a result of the memory-sensitive design of the Dalvik virtual machine - including the aforementioned compiler - the operating system makes the most out of the limited memory available to the mobile device. Android only requires a couple of hundred of kilobytes of heap space to fit the profiling system, just-in-time-compiler and the translation caches\cite{android_22}.

Moreover, the addition of a just-in-time compiler to Android resulted in a two- to six-fold speedup on a variety of CPU-intensive benchmarks compared with using the interpreter alone\cite{android_22}.
